{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34bff5ba-4272-4ae0-b0ff-b4c433406769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"HF_HOME\"] = \"/home/uw8/huggingface_cache\"\n",
    "os.environ[\"HF_DATASETS_OFFLINE\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "\n",
    "import outlines\n",
    "from outlines import models, generate\n",
    "\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "#from fuzzywuzzy import process\n",
    "\n",
    "from functools import partial\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77648ea1-755c-42d1-8b5b-623b1a43a86d",
   "metadata": {},
   "source": [
    "# Exp1 - Direct Generic Extraction (LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b2387fd-39d1-40cf-97db-395a15b4a437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CHECKPOINT_FILE = \"drug_extraction_checkpoint.csv\"\n",
    "DATA_FILE = \"sample_dummy_dataset.csv\"\n",
    "MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c69e215-09e5-48e6-a893-7c6953477716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-14 20:56:07 __init__.py:207] Automatically detected platform cuda.\n",
      "WARNING 06-14 20:56:07 config.py:2448] Casting torch.bfloat16 to torch.float16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-14 20:56:12 config.py:549] This model supports multiple tasks: {'generate', 'reward', 'classify', 'score', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 06-14 20:56:12 config.py:1382] Defaulting to use mp for distributed inference\n",
      "WARNING 06-14 20:56:12 cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 06-14 20:56:12 config.py:685] Async output processing is not supported on the current platform type cuda.\n",
      "INFO 06-14 20:56:12 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \n",
      "WARNING 06-14 20:56:12 multiproc_worker_utils.py:300] Reducing Torch parallelism from 16 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 06-14 20:56:12 custom_cache_manager.py:19] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2906359)\u001b[0;0m INFO 06-14 20:56:12 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "INFO 06-14 20:56:13 cuda.py:229] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2906359)\u001b[0;0m INFO 06-14 20:56:13 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 06-14 20:56:13 utils.py:916] Found nccl from library libnccl.so.2\n",
      "ERROR 06-14 20:56:13 pynccl_wrapper.py:229] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise, the nccl library might not exist, be corrupted or it does not support the current platform Linux-5.14.0-570.17.1.el9_6.x86_64-x86_64-with-glibc2.34.If you already have the library, please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2906359)\u001b[0;0m INFO 06-14 20:56:13 utils.py:916] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2906359)\u001b[0;0m ERROR 06-14 20:56:13 pynccl_wrapper.py:229] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise, the nccl library might not exist, be corrupted or it does not support the current platform Linux-5.14.0-570.17.1.el9_6.x86_64-x86_64-with-glibc2.34.If you already have the library, please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.\n",
      "INFO 06-14 20:56:13 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/uw8/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "WARNING 06-14 20:56:13 custom_all_reduce.py:145] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2906359)\u001b[0;0m INFO 06-14 20:56:13 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/uw8/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2906359)\u001b[0;0m WARNING 06-14 20:56:14 custom_all_reduce.py:145] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 06-14 20:56:14 shm_broadcast.py:258] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_797dc681'), local_subscribe_port=36011, remote_subscribe_port=None)\n",
      "INFO 06-14 20:56:14 model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2906359)\u001b[0;0m INFO 06-14 20:56:14 model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "INFO 06-14 20:56:14 weight_utils.py:254] Using model weights format ['*.safetensors', '*.bin', '*.pt']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2906359)\u001b[0;0m INFO 06-14 20:56:14 weight_utils.py:254] Using model weights format ['*.safetensors', '*.bin', '*.pt']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5cb0eb766634b018205a2e32e30a1b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=2906359)\u001b[0;0m INFO 06-14 20:56:17 model_runner.py:1115] Loading model weights took 7.5123 GB\n",
      "INFO 06-14 20:56:17 model_runner.py:1115] Loading model weights took 7.5123 GB\n"
     ]
    }
   ],
   "source": [
    "# Initialize model components\n",
    "llm = LLM(\n",
    "    model=MODEL_NAME,\n",
    "    dtype=\"float16\",\n",
    "    tensor_parallel_size=2,\n",
    "    gpu_memory_utilization=0.9,\n",
    "    max_model_len=4096,\n",
    "    enforce_eager=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f200208-e423-43a2-881c-a72fd5128dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema Definition\n",
    "DRUG_SCHEMA = '''{\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"generic_names\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\"type\": \"string\"},\n",
    "            \"minItems\": 0\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"generic_names\"]\n",
    "}'''\n",
    "\n",
    "\n",
    "def format_prompt(text: str) -> str:\n",
    "    return f\"\"\"Extract and normalize drug names to generic forms in JSON format. Rules:\n",
    "1. Convert brands to generics (Oncovin → vincristine)\n",
    "2. Expand abbreviations (MTX → methotrexate)\n",
    "3. Correct misspellings (Methotrxate → methotrexate)\n",
    "4. If a generic equivalent is unknown, include the raw drug name in lowercase.\n",
    "5. Use lowercase only\n",
    "\n",
    "Examples:\n",
    "Input: Administered Oncovin and IT MTX\n",
    "Output: {{\"generic_names\": [\"vincristine\", \"methotrexate\"]}}\n",
    "\n",
    "\n",
    "\n",
    "Process this text:\n",
    "{text}\n",
    "JSON Output:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "defdd73e-7ced-4e5f-9c35-e4492b2d2d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.2,\n",
    "    top_k=150,\n",
    "    top_p=0.6,\n",
    "    repetition_penalty=1.1,\n",
    "    max_tokens=8000,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "model = models.VLLM(llm)\n",
    "generator = generate.json(\n",
    "    model,\n",
    "    DRUG_SCHEMA,\n",
    "    whitespace_pattern=r\"[\\s]*\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92205f81-6dfe-4e3a-b51b-071ee9996d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def safe_json_loads(x):\n",
    "    \"\"\"Safely parse JSON with fallback\"\"\"\n",
    "    if pd.isna(x) or x.strip() in ['', '{}', '[]']:\n",
    "        return []\n",
    "    try:\n",
    "        return json.loads(x)\n",
    "    except json.JSONDecodeError:\n",
    "        return []\n",
    "\n",
    "def process_batch(batch_df):\n",
    "    prompts = [format_prompt(text) for text in batch_df[\"text_concat\"]]\n",
    "    results = []\n",
    "    \n",
    "    try:\n",
    "        responses = generator(prompts, sampling_params=sampling_params)\n",
    "        for response, (_, row) in zip(responses, batch_df.iterrows()):\n",
    "            try:\n",
    "                if isinstance(response, str):\n",
    "                    data = json.loads(response)\n",
    "                else:\n",
    "                    data = response\n",
    "                drugs = data.get(\"generic_names\", [])\n",
    "                results.append({\n",
    "                    \"unique_key\": row[\"unique_key\"],\n",
    "                    \"text_concat\": row[\"text_concat\"],\n",
    "                    \"json_extraction\": json.dumps(data, ensure_ascii=False),\n",
    "                    \"extracted_drugs\": drugs\n",
    "                })\n",
    "            except Exception as e:\n",
    "                results.append({\n",
    "                    \"unique_key\": row[\"unique_key\"],\n",
    "                    \"text_concat\": row[\"text_concat\"],\n",
    "                    \"json_extraction\": \"{}\",\n",
    "                    \"extracted_drugs\": []\n",
    "                })\n",
    "    except Exception as e:\n",
    "        print(f\"Batch failed: {str(e)[:200]}\")\n",
    "        for _, row in batch_df.iterrows():\n",
    "            results.append({\n",
    "                \"unique_key\": row[\"unique_key\"],\n",
    "                \"text_concat\": row[\"text_concat\"],\n",
    "                \"json_extraction\": \"{}\",\n",
    "                \"extracted_drugs\": []\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def load_checkpoint():\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        try:\n",
    "            df = pd.read_csv(\n",
    "                CHECKPOINT_FILE,\n",
    "                converters={\n",
    "                    'unique_drugs': safe_json_loads,\n",
    "                    'extracted_drugs': safe_json_loads,\n",
    "                    'json_extraction': safe_json_loads\n",
    "                }\n",
    "            )\n",
    "            if 'unique_key' not in df.columns:\n",
    "                raise ValueError(\"Corrupted checkpoint - missing columns\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Checkpoint reset due to error: {str(e)[:200]}\")\n",
    "            os.rename(CHECKPOINT_FILE, f\"{CHECKPOINT_FILE}.corrupted\")\n",
    "            return pd.DataFrame()\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def save_checkpoint(df):\n",
    "    df.to_csv(CHECKPOINT_FILE, index=False)\n",
    "\n",
    "def compute_metrics(row):\n",
    "    true_drugs = set(row['unique_drugs'])\n",
    "    pred_drugs = set(row['extracted_drugs'])\n",
    "    \n",
    "    tp = len(true_drugs & pred_drugs)\n",
    "    precision = tp / len(pred_drugs) if pred_drugs else 0\n",
    "    recall = tp / len(true_drugs) if true_drugs else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) else 0\n",
    "    \n",
    "    return pd.Series({\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"missing_drugs\": list(true_drugs - pred_drugs),\n",
    "        \"hallucinated_drugs\": list(pred_drugs - true_drugs)\n",
    "    })\n",
    "\n",
    "def print_avg_metrics(df):\n",
    "    if not df.empty and 'precision' in df.columns:\n",
    "        print(\"\\nAverage Metrics Across All Processed Records:\")\n",
    "        print(f\"Precision: {df['precision'].mean():.4f}\")\n",
    "        print(f\"Recall:    {df['recall'].mean():.4f}\")\n",
    "        print(f\"F1 Score:  {df['f1'].mean():.4f}\")\n",
    "    else:\n",
    "        print(\"\\nNo metrics available - empty dataset or missing columns\")\n",
    "\n",
    "def run_pipeline():\n",
    "    df = load_checkpoint()\n",
    "    raw_data = pd.read_csv(DATA_FILE)\n",
    "    \n",
    "    # Create unique key first\n",
    "    raw_data[\"unique_key\"] = (\n",
    "        raw_data[\"patient_id_number\"].astype(str) + \"_\" +\n",
    "        raw_data[\"tumor_record_number\"].astype(str) + \"_\" +\n",
    "        raw_data[\"admission_id\"].astype(str)\n",
    "    )\n",
    "    raw_data[\"unique_drugs\"] = raw_data[\"unique_drugs\"].apply(\n",
    "        lambda x: x.split(\", \") if isinstance(x, str) else []\n",
    "    )\n",
    "    \n",
    "    # Find unprocessed records\n",
    "    processed_keys = set(df[\"unique_key\"]) if not df.empty else set()\n",
    "    todo = raw_data[~raw_data[\"unique_key\"].isin(processed_keys)]\n",
    "    \n",
    "    if todo.empty:\n",
    "        print(\"All data processed\")\n",
    "        print_avg_metrics(df)\n",
    "        return df\n",
    "    \n",
    "    print(f\"Processing {len(todo)} new records...\")\n",
    "    \n",
    "    # Process in batches\n",
    "    batch_size = 32\n",
    "    for i in range(0, len(todo), batch_size):\n",
    "        batch = todo.iloc[i:i+batch_size].copy()\n",
    "        batch_results = process_batch(batch)\n",
    "        \n",
    "        # Merge results\n",
    "        merged = pd.merge(\n",
    "            batch,\n",
    "            batch_results,\n",
    "            on=[\"unique_key\", \"text_concat\"],\n",
    "            how=\"left\"\n",
    "        )\n",
    "        \n",
    "        # Calculate metrics\n",
    "        if not merged.empty:\n",
    "            merged[[\"precision\", \"recall\", \"f1\", \"missing_drugs\", \"hallucinated_drugs\"]] = \\\n",
    "                merged.apply(compute_metrics, axis=1)\n",
    "        \n",
    "        # Update and save incrementally\n",
    "        df = pd.concat([df, merged], ignore_index=True)\n",
    "        save_checkpoint(df)\n",
    "        print(f\"Processed batch {i//batch_size + 1}/{(len(todo)//batch_size)+1}\")\n",
    "    \n",
    "    print_avg_metrics(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ac47bdd-1bc3-439c-9072-e98ade34ed7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 100 new records...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 32/32 [01:13<00:00,  2.29s/it, est. speed input: 70.95 toks/s, output: 68.17 toks/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch failed: Unterminated string starting at: line 1 column 11814 (char 11813)\n",
      "Processed batch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 32/32 [00:09<00:00,  3.21it/s, est. speed input: 520.21 toks/s, output: 111.02 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 32/32 [00:10<00:00,  3.15it/s, est. speed input: 513.56 toks/s, output: 112.01 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 4/4 [01:06<00:00, 16.52s/it, est. speed input: 9.80 toks/s, output: 61.32 toks/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch failed: Expecting value: line 1 column 10568 (char 10567)\n",
      "Processed batch 4/4\n",
      "\n",
      "Average Metrics Across All Processed Records:\n",
      "Precision: 0.4957\n",
      "Recall:    0.4329\n",
      "F1 Score:  0.4571\n",
      "\n",
      "Pipeline completed. Sample output:\n",
      "    unique_key                                                                                                                                     text_concat extracted_drugs\n",
      "8270_3570_7659 The regimen included CHOP along with Methotrexate and Prednisone. Added Prednisone for hormonal therapy. Prescribed Caplacizumab and Rituximab.              []\n",
      "1860_7056_4291            Administered IT MTX and Doxorubicin. Added Prednisone for hormonal therapy. Patient received Atezolizumab for lung cancer treatment.              []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    final_df = run_pipeline()\n",
    "    print(\"\\nPipeline completed. Sample output:\")\n",
    "    if not final_df.empty:\n",
    "        print(final_df[[\"unique_key\", \"text_concat\", \"extracted_drugs\"]].head(2).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2f57d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95db3ed0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
